{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Player's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                       # Regular Expressison Library\n",
    "import time                     # To introduce time delay\n",
    "import json\n",
    "import requests                 # To get the content of the web page\n",
    "import pandas as pd             # Library to create Dataframe\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup, Comment   # The data scrapping library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "base_url = 'https://www.basketball-reference.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the team urls from the file\n",
    "with open('.\\\\data\\\\season_urls.json', 'r', encoding='utf8') as file_obj:\n",
    "    season_urls = json.load(file_obj or {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player's Name and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Washington Wizards: 100%|████████████████████████████████████████████████████████████| 420/420 [18:35<00:00,  2.60s/it]"
     ]
    }
   ],
   "source": [
    "# Set to store player's name and URL\n",
    "player_urls = {}\n",
    "\n",
    "pbar = tqdm(total=len(season_urls)*14, desc='Progress')\n",
    "\n",
    "for season in season_urls:\n",
    "    pbar.set_description(season)\n",
    "    \n",
    "    for url in season_urls.get(season):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Get request to each team page\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Create Beautiful Soup Object\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        # Getting roster table from the website\n",
    "        roster = soup.find('div', {'id': 'div_roster'}).tbody.find_all('td', {'data-stat': 'player'})\n",
    "        \n",
    "        # Adding player name and URLs to the dictionary \n",
    "        for player in roster:\n",
    "            name = re.sub('\\s{2,}\\(TW\\)', '', player.text.strip())\n",
    "            player_urls[player.text.strip()] = base_url+player.a['href']\n",
    "\n",
    "        # Due to rate limit\n",
    "        time.sleep(2)\n",
    "\n",
    "# Writing the player URLs into a file\n",
    "with open('.\\\\data\\\\player_urls.json', 'w', encoding='utf8') as file_obj:\n",
    "    json.dump(player_urls, file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player's Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Donovan Williams  (TW):   1%|▌                                                    | 17/1675 [21:31<34:58:34, 75.94s/it]\n",
      "Nate Hinton:  42%|███████████████████████████▉                                      | 709/1675 [33:22<43:16,  2.69s/it]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(player_urls), desc='Player Name')\n",
    "dnf = {}\n",
    "\n",
    "for player in player_urls:\n",
    "    pbar.set_description(player)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Due to rate limitation\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Getting the player page data\n",
    "    response = requests.get(player_urls.get(player))\n",
    "    \n",
    "    # Creating Beautiful Soup object\n",
    "    adv_soup = BeautifulSoup(response.content, 'lxml')\n",
    "            \n",
    "    # Storing the Advanced Stats into a Pandas' Dataframe\n",
    "    adv_table = adv_soup.find(id='advanced')\n",
    "    if adv_table:\n",
    "        adv = pd.read_html(str(adv_table))[0]\n",
    "        adv.drop(['Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'MP'], axis=1, inplace=True)\n",
    "    else:\n",
    "        dnf[player] = player_urls.get(player)\n",
    "        continue\n",
    "        \n",
    "    stats_soup = BeautifulSoup('\\n'.join(adv_soup.find_all(string=Comment)), 'lxml')\n",
    "\n",
    "    # Storing the stats into a Pandas' Dataframe\n",
    "    stats = pd.read_html(str(stats_soup.find(id='per_poss')))[0]\n",
    "\n",
    "    # Creating a final dataframe\n",
    "    total_stats = pd.concat([stats, adv], axis=1)\n",
    "    total_stats.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    file_name = re.sub('\\.\\s|\\.|\\s|-', '_', player.strip().lower())\n",
    "\n",
    "    # Writing the player's stats into a CSV file\n",
    "    total_stats.to_csv('.\\\\data\\\\stats\\\\'+file_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\\\data\\\\data_not_found.json', 'w', encoding='utf8') as file_obj:\n",
    "    json.dump(dnf, file_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "70310534a0af6c5047c15b9ecd8d94517e75e84f57266693a2902a1547b9c295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
